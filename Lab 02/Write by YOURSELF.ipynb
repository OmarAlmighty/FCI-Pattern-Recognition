{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IDLE configuration:\n",
    "1.  Open Anaconda\n",
    "2.  Launch the CMD\n",
    "3.  execute `idle`\n",
    "4.  In the python shell, execute:\n",
    "\n",
    "    `import sys`\n",
    "    \n",
    "    `sys.path.append('F:\\\\fci\\\\Pattern Recognation\\\\Week 03 - Introduction')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Introdcution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prml.linear import (\n",
    "    LinearRegression, RidgeRegression, BayesianRegression\n",
    ")\n",
    "from prml.preprocess import PolynomialFeature\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Example: Polynomial Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the training dataset and test dataset\n",
    "\n",
    "**Steps**\n",
    "1.  Read the training dataset: `train_set = np.genfromtxt('Train_dataset.csv', delimiter=',')`\n",
    "\n",
    "2.  Read the test dataset: `test_set = np.genfromtxt('Test_dataset.csv', delimiter=',')`\n",
    "\n",
    "3.  Split the training dataset into features (x) and targets (y): `x_train, y_train = train_set[:, 0], train_set[:, 1]`\n",
    "\n",
    "4.  Split the test dataset into features (x) and targets (y): `x_test, y_test = test_set[:, 0], test_set[:, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training set and the test set\n",
    "\n",
    "**Steps**\n",
    "1.  Plot the training dataset using scatters: `plt.scatter(feature, target, facecolor, edgecolors, s, label)`\n",
    "\n",
    "2.  Plot the test dataset using a line plot: `plt.plot(feature, target, color, label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a linear regression model\n",
    "\n",
    "$\\Large y(x, \\textbf{w}) = w_0 + w_1 x^1 + w_2 x^2 + w_3 x^3 + ... + w_M x^M = \\sum_{j=0}^{M}{w_j x^j}$\n",
    "\n",
    "**Steps**\n",
    "1.  Define the degree of the model: `degree = ...`\n",
    "\n",
    "2.  Transform the features of the training set and test set to the degree of the model:\n",
    "    *   `feature = PolynomialFeature(degree)`\n",
    "    *   `X_train = feature.transform(x_train)`\n",
    "    *   `X_test = feature.transform(x_test)`<br>\n",
    "    <br> \n",
    "3.  Define a linear regression model: `model = LinearRegression()`\n",
    "\n",
    "4.  Fit the model to the training set: `model.fit(X_train, y_train)`\n",
    "\n",
    "5.  Test the model on the features of the test set: `y = model.predict(X_test)`\n",
    "\n",
    "6.  Plot the training dataset using scatters: `plt.scatter(feature, target, facecolor, edgecolors, s, label)`\n",
    "    *   ***Note that we plot the original features not the transformed features*** <br>\n",
    "    <br>\n",
    "7.  Plot the test dataset using line plot: `plt.plot(feature, target, c, label)`\n",
    "\n",
    "8.  Plot the model's prediction as a line plot: `plt.plot(feature, predictions, c, label)`\n",
    "\n",
    "9.  Set figure limits: `plt.ylim(-1.5, 1.5)`\n",
    "\n",
    "10. Compare how similar the predictions to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different model's degrees in one figure and print the weights of each model\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1.  Loop through the degrees: `for i, degree in enumerate([0, 1, 3, 9]):`\n",
    "\n",
    "    1.  Create a subplot: `plt.subplot(number of rows, number of columns, index)`\n",
    "\n",
    "    2.  Transform the training set features and test set features to the required degree:\n",
    "        *   `feature = PolynomialFeature(degree)`\n",
    "        *   `X_train = feature.transform(x_train)`\n",
    "        *   `X_test = feature.transform(x_test)` <br>\n",
    "        <br>\n",
    "    3.  Create a linear regression model: `model = LinearRegression()`\n",
    "\n",
    "    4.  Fit the model to the training dataset: `model.fit(X_train, y_train)`\n",
    "\n",
    "    5.  Print the weights of the model: `print(\"Degree \", degree, \"model: \", model.w)`\n",
    "\n",
    "    6.  Test the model on the features of the test dataset: `y = model.predict(X_test)`\n",
    "\n",
    "    7.  Plot the training dataset using scatters: `plt.scatter(feature, target, facecolor, edgecolors, s, label)`\n",
    "        *   ***Note that we plot the original features not the transformed features*** <br>\n",
    "        <br>\n",
    "    8.  Plot the test dataset using line plot: `plt.plot(feature, target, c, label)`\n",
    "\n",
    "    9.  Plot the model's prediction as a line plot: `plt.plot(feature, predictions, c, label)`\n",
    "\n",
    "    10.  Set figure limits: `plt.ylim(-1.5, 1.5)`\n",
    "\n",
    "`plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Error\n",
    "\n",
    "We will compute the error using Root Mean Squared Error (RMSE) function.\n",
    "\n",
    "$ \\LARGE \\sqrt{\\frac{\\sum_{t=1}^T({\\hat{y_t} - y_t})^2}{T}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1.  Define the error function:\n",
    "    ```\n",
    "    def rmse(a, b):\n",
    "        return np.sqrt(np.mean(np.square(a - b)))\n",
    "    ```\n",
    "2.  Create a list for the training errors: `train_errors = []`\n",
    "\n",
    "3.  Create a list for the test errors: `test_errors = []`\n",
    "\n",
    "4.  Loop through 10 models' degrees: `for i in range(10):`\n",
    "\n",
    "    1.  Transform the features into degree `i`:\n",
    "\n",
    "        ```\n",
    "        feature = PolynomialFeature(i)\n",
    "        X_train = feature.transform(x_train)\n",
    "        X_test = feature.transform(x_test)\n",
    "        ```\n",
    "    2.  Create a linear regression model: `model = LinearRegression()`\n",
    "\n",
    "    3.  Fit a linear regression model to the training dataset: `model.fit(X_train, y_train)`\n",
    "\n",
    "    4.  Test the model on the features of the training set: `train_preds = model.predict(X_train)`\n",
    "\n",
    "    5.  Test the model on the features of the test set: `test_preds = model.predict(X_test)`\n",
    "\n",
    "    6.  Compute the error on the training set: `train_err = rmse(train_preds, y_train)`\n",
    "\n",
    "    7.  Compute the error on the test set: `test_err = rmse(test_preds, y_test)`\n",
    "\n",
    "    8.  Print the error values:\n",
    "        ```\n",
    "        print(\"\\nDegree: \", degree)\n",
    "        print(\"\\tTraining error: \", train_err)\n",
    "        print(\"\\tTest errors: \", test_err)\n",
    "        ```\n",
    "    9.  Append the training error to the list: `training_errors.append(train_err)`\n",
    "\n",
    "    10.  Append the test error to the list: `test_errors.append(test_err)`\n",
    "\n",
    "5.  Plot the training and test errors as line plot:\n",
    "    ```\n",
    "    plt.plot(training_errors, 'o-', mfc=\"none\", mec=\"b\", ms=10, c=\"b\", label=\"Training\")\n",
    "    plt.plot(test_errors, 'o-', mfc=\"none\", mec=\"r\", ms=10, c=\"r\", label=\"Test\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulariztion\n",
    "\n",
    "Regularization is a penalty term added to the error function to prevent coefficient from reaching large values.\n",
    "\n",
    "To define a linear regression model with regulariztion, we will use RidgeRegression model. \n",
    "\n",
    "This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm.\n",
    "\n",
    "$\\Large E(\\bf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{{y(x_n, \\bf{w}) - t_n}\\}^2 + \\frac{\\lambda}{2} ||\\bf{w}||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "1.  Transform the features to degree 9 to create an overfitted model:\n",
    "    ```\n",
    "    feature = PolynomialFeature(9)\n",
    "    X_train = feature.transform(x_train)\n",
    "    X_tet = feature.transform(x_test)\n",
    "    ```\n",
    "2.  Create a ridge regression model: `model = RidgeRegression(alpha=1e-4)`\n",
    "    *   `alpha` is the inverse of $\\lambda$. <br>\n",
    "    <br>\n",
    "\n",
    "3.  Fit the model to the training set: `model.fit(X_train, y_train)`\n",
    "\n",
    "4.  Test the model on the features of the test set: `y = model.predict(X_test)`\n",
    "\n",
    "5.  Print the RMSE of the model: `print(\"RMSE: \", rmse(preds, y_test))`\n",
    "\n",
    "6.  Plot the training set as scatters: `plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")`\n",
    "\n",
    "7.  Plot the test set as line plot: `plt.plot(x_test, y_test, c=\"g\", label=\"Test dataset\")`\n",
    "\n",
    "8.  Plot the model's prediction as line plot: `plt.plot(x_test, y, c=\"r\", label=\"fitting\")`\n",
    "\n",
    "9.  Set figure limit: `plt.ylim(-1.5, 1.5)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.6 Bayesian curve fitting\n",
    "\n",
    "We will apply a regression model that implements the Baysian formula \n",
    "\n",
    "$\\Large ùëù(\\bf{ùíò‚îÇùíô,ùíï},\\alpha,\\beta) = ùëù(\\bf{ùíï‚îÇùíô,ùíò},\\beta)ùëù(\\bf{ùíò}‚îÇ\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1.  Transform the features to degree 9 to create an overfitted model:\n",
    "    ```\n",
    "    feature = PolynomialFeature(9)\n",
    "    X_train = feature.transform(x_train)\n",
    "    X_tet = feature.transform(x_test)\n",
    "    ```\n",
    "2.  Create a bayesian regression model: `model = BayesianRegression(alpha=2e-3, beta=2)`\n",
    "\n",
    "3.  Fit the model to the training dataset: `model.fit(X_train, y_train)`\n",
    "\n",
    "4.  Test the model on the features of the test set: `y, std = model.predict(X_test, return_std=True)`\n",
    "    *   Notice that we return the standard deviation of the model.<br>\n",
    "    <br>\n",
    "\n",
    "5.  Plot the training set as scatters: `plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")`\n",
    "\n",
    "6.  Plot the test set as line plot: `plt.plot(x_test, y_test, c=\"g\", label=\"Test dataset\")`\n",
    "\n",
    "7.  Plot the model's prediction as line plot: `plt.plot(x_test, y, c=\"r\", label=\"fitting\")`\n",
    "\n",
    "8.  Visualize the standard deviation of the predictive distribution: `plt.fill_between(x_test, y - std, y + std, color=\"pink\", label=\"std.\", alpha=0.5)`\n",
    "\n",
    "`plt.legend(bbox_to_anchor=(1.05, 1.), loc=2, borderaxespad=0.)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the dataset is created?\n",
    "\n",
    "## The training dataset \n",
    "\n",
    "*   It consists of 10 samples.\n",
    "\n",
    "*   The features are real-numbers ranging from 0 to 1.\n",
    "\n",
    "*   The target values are computed using: $\\sin{2*\\pi*x}$.\n",
    "\n",
    "*   We added random normal noise to the target values.\n",
    "\n",
    "## The test dataset\n",
    "\n",
    "*   It consists of 100 samples.\n",
    "\n",
    "*   The features are real-numbers ranging from 0 to 1.\n",
    "\n",
    "*   The target values are computed using: $\\sin{(2*\\pi*x)}$.\n",
    "\n",
    "*   **No noise added to stimulate the how precise our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an artificial dataset with added gaussian noise\n",
    "def create_dataset(func, sample_size, std):\n",
    "    # create features of values between 0 and 1 with size sample_size\n",
    "    x = np.linspace(0, 1, sample_size)\n",
    "    # create the target vector computed from the function func with added noise\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function sin(2*pi*x) that resembles the target values\n",
    "def my_sin(x):\n",
    "    return np.sin(2 * np.pi * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training dataset (x_train, y_train), we will create 10 samples with standard deviation 0.25\n",
    "x_train, y_train = create_dataset(my_sin, 10, 0.25)\n",
    "\n",
    "# generate test dataset with 100 samples that resembles the true underlying function\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "y_test = my_sin(x_test)\n",
    "\n",
    "trains = np.transpose(np.array([x_train, y_train]))\n",
    "np.savetxt('Train dataset.csv',trains, delimiter=',')\n",
    "tests = np.transpose(np.array([x_test, y_test]))\n",
    "np.savetxt('Test dataset.csv',tests, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4ffc9c9a031070cdf645d18822cd1abb711111454341ec96112cbb04136171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
